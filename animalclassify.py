# -*- coding: utf-8 -*-
"""AnimalClassify.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AIhEquHhvbTjEKrjKT-zC5-Dqs4VClFR
"""

from google.colab import drive
drive.mount('/content/drive')
!unzip "/content/drive/MyDrive/archive.zip" -d "/content/animals_dataset"

!ls "/content/animals_dataset"

import os
import shutil
import random

original_dataset_dir = "/content/animals_dataset/Animals-10"
base_dir = "/content/animals_split"  # New folder to hold train/val split
os.makedirs(base_dir, exist_ok=True)

train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'validation')
os.makedirs(train_dir, exist_ok=True)
os.makedirs(validation_dir, exist_ok=True)

# Get list of class names (folders)
classes = os.listdir(original_dataset_dir)

# Split data for each class
for class_name in classes:
    class_path = os.path.join(original_dataset_dir, class_name)
    images = os.listdir(class_path)
    random.shuffle(images)

    train_class_dir = os.path.join(train_dir, class_name)
    val_class_dir = os.path.join(validation_dir, class_name)
    os.makedirs(train_class_dir, exist_ok=True)
    os.makedirs(val_class_dir, exist_ok=True)

    split_idx = int(0.8 * len(images))  # 80-20 split
    train_images = images[:split_idx]
    val_images = images[split_idx:]

    for fname in train_images:
        src = os.path.join(class_path, fname)
        dst = os.path.join(train_class_dir, fname)
        shutil.copyfile(src, dst)

    for fname in val_images:
        src = os.path.join(class_path, fname)
        dst = os.path.join(val_class_dir, fname)
        shutil.copyfile(src, dst)

print("Dataset split done.")

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Paths to train and validation sets
train_dir = "/content/animals_split/train"
validation_dir = "/content/animals_split/validation"

# Rescale the pixel values from [0, 255] to [0, 1]
train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)

# Load training data
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode='categorical'  # for multi-class classification
)

# Load validation data
validation_generator = val_datagen.flow_from_directory(
    validation_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode='categorical'
)

import tensorflow as tf
from tensorflow.keras import layers, models

model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),
    layers.MaxPooling2D(2, 2),

    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D(2, 2),

    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D(2, 2),

    layers.Flatten(),
    layers.Dense(512, activation='relu'),
    layers.Dense(10, activation='softmax')  # 10 classes
])

# Compile the model
model.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

# Model summary
model.summary()
history = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator),
    epochs=10,
    validation_data=validation_generator,
    validation_steps=len(validation_generator)
)

import matplotlib.pyplot as plt

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs_range = range(len(acc))

plt.figure(figsize=(14, 5))

plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training vs Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training vs Validation Loss')

plt.show()

from google.colab import files
uploaded = files.upload()
import numpy as np
from tensorflow.keras.preprocessing import image
import matplotlib.pyplot as plt

# Load the uploaded image
img_path = list(uploaded.keys())[0]  # Get the uploaded image file name
img = image.load_img(img_path, target_size=(150, 150))

# Convert the image to an array and normalize it
img_array = image.img_to_array(img) / 255.0
img_array = np.expand_dims(img_array, axis=0)

# Make prediction
prediction = model.predict(img_array)
predicted_class = list(train_generator.class_indices.keys())[np.argmax(prediction)]

# Display the image and prediction result
plt.imshow(img)
plt.title(f"Predicted: {predicted_class}")
plt.axis('off')
plt.show()

print(f"Prediction: {predicted_class}")

# Save the model to a .h5 file
model.save('/content/animals_model.h5')

# Move the model to Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Save the model to your Google Drive
# Save the model in the new Keras format
model.save('/content/drive/MyDrive/animals_model.keras')

from tensorflow.keras.models import load_model
model = load_model('/content/drive/MyDrive/animals_model.h5')